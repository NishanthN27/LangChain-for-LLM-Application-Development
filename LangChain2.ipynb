{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV35fU-Cz0f5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOGETHER_API_KEY\"]=\"YOUR_TOGETHER_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "uOe5S0bUz3hW"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade langchain\n",
        "#%pip install -qU langchain-together\n",
        "#!pip install together\n",
        "#!pip install -U langchain-community\n",
        "#!pip install -U langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PByn3A5e0OgM"
      },
      "outputs": [],
      "source": [
        "from together import Together\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "from langchain_together import ChatTogether\n",
        "llm=ChatTogether(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "    temperature=0\n",
        ")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory = memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "0iFE9ICy0asv",
        "outputId": "9dd3ca96-b3ca-41d8-f75b-288a76665410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Nishanth\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello Nishanth, it's a pleasure to meet you. I'm an AI designed to engage in conversations and provide information on a wide range of topics. I've been trained on a vast amount of text data, including books, articles, and online content, which allows me to generate human-like responses. My training data is based on a massive corpus of text that was cut off in 2023, so I may not be aware of very recent events or developments. I'm excited to chat with you, Nishanth, and see where our conversation takes us. What brings you here today? Would you like to talk about a specific topic, or just see where the conversation goes?\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Nishanth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "p6g032bI1fxV",
        "outputId": "8c26bc11-0c57-42e5-d473-39e376cb8e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Nishanth\n",
            "AI: Hello Nishanth, it's a pleasure to meet you. I'm an AI designed to engage in conversations and provide information on a wide range of topics. I've been trained on a vast amount of text data, including books, articles, and online content, which allows me to generate human-like responses. My training data is based on a massive corpus of text that was cut off in 2023, so I may not be aware of very recent events or developments. I'm excited to chat with you, Nishanth, and see where our conversation takes us. What brings you here today? Would you like to talk about a specific topic, or just see where the conversation goes?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Nishanth, that's a simple yet classic question. The answer to 1+1 is, of course, 2. This is a basic arithmetic operation that is widely used in mathematics, and it's a fundamental concept that is taught to children from a very young age. In fact, the concept of addition, including the equation 1+1, has been a cornerstone of mathematics for thousands of years, dating back to ancient civilizations such as the Babylonians, Egyptians, and Greeks. Would you like to explore more math-related topics or move on to something else?\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "k6uhJcio1xwl",
        "outputId": "2f46f3d6-df74-46c7-aed3-e39ce1a52ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Nishanth\n",
            "AI: Hello Nishanth, it's a pleasure to meet you. I'm an AI designed to engage in conversations and provide information on a wide range of topics. I've been trained on a vast amount of text data, including books, articles, and online content, which allows me to generate human-like responses. My training data is based on a massive corpus of text that was cut off in 2023, so I may not be aware of very recent events or developments. I'm excited to chat with you, Nishanth, and see where our conversation takes us. What brings you here today? Would you like to talk about a specific topic, or just see where the conversation goes?\n",
            "Human: What is 1+1?\n",
            "AI: Nishanth, that's a simple yet classic question. The answer to 1+1 is, of course, 2. This is a basic arithmetic operation that is widely used in mathematics, and it's a fundamental concept that is taught to children from a very young age. In fact, the concept of addition, including the equation 1+1, has been a cornerstone of mathematics for thousands of years, dating back to ancient civilizations such as the Babylonians, Egyptians, and Greeks. Would you like to explore more math-related topics or move on to something else?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Nishanth, I remember that you introduced yourself at the beginning of our conversation. Your name is Nishanth. I'm designed to keep track of context and recall specific details from our conversation, so I can easily remember your name. If you'd like, I can also address you by your name throughout our conversation to make it feel more personal and friendly. Would you like to talk about something related to your name, or is there something else on your mind that you'd like to discuss?\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR2AxOqH12ze",
        "outputId": "d06e1856-03f2-4996-fb37-c3b03d521401"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Nishanth\\nAI: Hello Nishanth, it's a pleasure to meet you. I'm an AI designed to engage in conversations and provide information on a wide range of topics. I've been trained on a vast amount of text data, including books, articles, and online content, which allows me to generate human-like responses. My training data is based on a massive corpus of text that was cut off in 2023, so I may not be aware of very recent events or developments. I'm excited to chat with you, Nishanth, and see where our conversation takes us. What brings you here today? Would you like to talk about a specific topic, or just see where the conversation goes?\\nHuman: What is 1+1?\\nAI: Nishanth, that's a simple yet classic question. The answer to 1+1 is, of course, 2. This is a basic arithmetic operation that is widely used in mathematics, and it's a fundamental concept that is taught to children from a very young age. In fact, the concept of addition, including the equation 1+1, has been a cornerstone of mathematics for thousands of years, dating back to ancient civilizations such as the Babylonians, Egyptians, and Greeks. Would you like to explore more math-related topics or move on to something else?\\nHuman: What is my name?\\nAI: Nishanth, I remember that you introduced yourself at the beginning of our conversation. Your name is Nishanth. I'm designed to keep track of context and recall specific details from our conversation, so I can easily remember your name. If you'd like, I can also address you by your name throughout our conversation to make it feel more personal and friendly. Would you like to talk about something related to your name, or is there something else on your mind that you'd like to discuss?\"}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1Fww3XB-189X"
      },
      "outputs": [],
      "source": [
        "memory=ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0bP120aH2A5Q"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RYsqXNZ2DTM",
        "outputId": "875f4336-ce98-4d81-f5d1-e6fc85882bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOyzC-mx2Et9",
        "outputId": "7bb1756c-f8b2-470a-ad01-2a525a1ef859"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QJbGAcLX2HSW"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lmy6JL_2JKI",
        "outputId": "9ea90f22-3be8-4477-a400-44e545815443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo0Cf0kSIAhf",
        "outputId": "af3e01c4-142e-4611-8f77-e9b5585a6651"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-39-a4a19b3401fd>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationBufferWindowMemory(k=1)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory=ConversationBufferWindowMemory(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "FJPjtm3WINiF"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mVOvaOnIRRu",
        "outputId": "bef8892d-3fd2-4414-a85e-9cb2af420db1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vsjZfJ3gITCn"
      },
      "outputs": [],
      "source": [
        "llm=ChatTogether(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "PcC1tZ__IYby"
      },
      "outputs": [],
      "source": [
        "memory=ConversationBufferWindowMemory(k=1) # just remember 1 conversational exchange\n",
        "conversation=ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "Bt8xtrh9IdKH",
        "outputId": "ae38636d-ec13-4f4b-fd22-daca95ccbdf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Nishanth\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello Nishanth, it's a pleasure to meet you. I'm an AI designed to engage in conversations and provide information on a wide range of topics. I've been trained on a vast amount of text data, including books, articles, and online content, which allows me to generate human-like responses. My training data is based on a massive corpus of text that was cut off in 2023, so I may not be aware of very recent events or developments. I'm excited to chat with you and see where our conversation takes us. What brings you here today, Nishanth? Are you interested in discussing a particular topic, or would you like me to suggest some conversation starters?\""
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Nishanth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "OEjK_qhKIfGx",
        "outputId": "bb14ba1c-7dca-44c5-bbd4-848ce3875f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Nishanth\n",
            "AI: Hello Nishanth, it's a pleasure to meet you. I'm an AI designed to engage in conversations and provide information on a wide range of topics. I've been trained on a vast amount of text data, including books, articles, and online content, which allows me to generate human-like responses. My training data is based on a massive corpus of text that was cut off in 2023, so I may not be aware of very recent events or developments. I'm excited to chat with you and see where our conversation takes us. What brings you here today, Nishanth? Are you interested in discussing a particular topic, or would you like me to suggest some conversation starters?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Nishanth, that's a simple yet classic question. The answer to 1+1 is, of course, 2. This is a fundamental concept in mathematics, and it's a great example of a basic arithmetic operation. In fact, the concept of addition, including the equation 1+1, has been a cornerstone of mathematics for thousands of years, dating back to ancient civilizations such as the Babylonians, Egyptians, and Greeks. If you're interested, I can provide more information on the history of mathematics or help you with more complex math problems. Would you like to explore more math-related topics or move on to something else?\""
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "dOUXwzdcIlEL",
        "outputId": "af2ba383-b9dd-47ae-b5d4-dd6c59fab341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is 1+1?\n",
            "AI: Nishanth, that's a simple yet classic question. The answer to 1+1 is, of course, 2. This is a fundamental concept in mathematics, and it's a great example of a basic arithmetic operation. In fact, the concept of addition, including the equation 1+1, has been a cornerstone of mathematics for thousands of years, dating back to ancient civilizations such as the Babylonians, Egyptians, and Greeks. If you're interested, I can provide more information on the history of mathematics or help you with more complex math problems. Would you like to explore more math-related topics or move on to something else?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You\\'ve asked me to recall your name, but unfortunately, I don\\'t have that information. In our conversation so far, you haven\\'t mentioned your name, and I don\\'t have any prior knowledge about your identity. I can tell you that earlier, I addressed you with the name \"Nishanth,\" but that was an error on my part, and I don\\'t actually know your name. If you\\'d like to share it with me, I\\'d be happy to know it and use it in our conversation.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WFDqLwuHIoUB",
        "outputId": "49b7da92-0164-449b-c78d-48c3203e4c45"
      },
      "outputs": [],
      "source": [
        "#!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "x1F0FH8TIv0E"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain_together import ChatTogether\n",
        "import tiktoken\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "# Extend the ChatTogether class to add the missing method\n",
        "class MyChatTogether(ChatTogether):\n",
        "    def get_num_tokens_from_messages(self, messages, **kwargs):\n",
        "        \"\"\"\n",
        "        Calculate the number of tokens in a list of messages.\n",
        "        \"\"\"\n",
        "        num_tokens=0\n",
        "        for message in messages:\n",
        "            num_tokens+=len(encoding.encode(message.content))\n",
        "        return num_tokens\n",
        "\n",
        "# Using extended class to create llm\n",
        "llm=MyChatTogether(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=30, encoding=encoding)\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMXKAu5pIxqT",
        "outputId": "e38edfd9-e0db-41c9-c0c8-0616ade1b04b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "fSonURvpKMIa"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FgV2LiaKUbb",
        "outputId": "b02b59d8-c9e1-4081-e180-92e26cd46f4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-66-11fc0fbb5701>:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
          ]
        }
      ],
      "source": [
        "schedule=\"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHljBJfPKZI4",
        "outputId": "a4108261-2bfd-4530-b796-e36661f733c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'System: The human greets the AI, and they exchange casual greetings, with the human eventually asking about the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "t4Zf9cwEKbYR"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "gFZJtB61KhIu",
        "outputId": "2de99987-b2ef-4745-88c7-85581751fd46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI, and they exchange casual greetings, with the human eventually asking about the schedule for the day.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"For the demo, I would recommend showing the latest advancements in natural language processing, specifically the capabilities of the LLaMA model. You could demonstrate its ability to understand and respond to complex questions, as well as its text generation capabilities. Perhaps you could even show how it can be fine-tuned for specific tasks, such as text summarization or sentiment analysis.\\n\\nAdditionally, you could also showcase the integration of the LLaMA model with other tools and platforms, such as the ability to generate text based on a given prompt, or to converse with the model in a more interactive way. This would give the customer a better understanding of the potential applications and use cases of the technology.\\n\\nIf you'd like, I can also provide you with some specific examples and prompts that you could use to demonstrate the capabilities of the model. For example, you could ask the model to generate a short story based on a given prompt, or to summarize a longer piece of text into a shorter summary. I have a few examples prepared, including a demo of the model's ability to generate text in different styles, such as a news article or a product description. Would you like me to share those with you?\""
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIfXkHlZKjEU",
        "outputId": "fd8e7c44-f450-4abd-b5ab-982b4042e63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI, and they exchange casual greetings, with the human eventually asking about the schedule for the day. The AI informs the human of their schedule, which includes a meeting at 8am with the product team, time to work on the LangChain project from 9am-12pm, and lunch at an Italian restaurant with a customer at Noon, where the human will demo the latest LLM advancements. The AI recommends demonstrating the LLaMA model's capabilities in natural language processing, such as understanding and responding to complex questions, text generation, and fine-tuning for specific tasks like text summarization or sentiment analysis, and also suggests showcasing the model's integration with other tools and platforms, offering to provide examples and prompts to aid in the demo.\"}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
